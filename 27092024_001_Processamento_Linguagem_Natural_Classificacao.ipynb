{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo 02: Previsão de feedbacks de produtos B2W** \n",
    "\n",
    " \n",
    "\n",
    "Você recebeu um convite para uma consultoria, na qual deve desenvolver um modelo de previsões de feedbacks de clientes em produtos comprados na loja, que serão coletados do instagram. \n",
    "\n",
    " \n",
    "\n",
    "Os dados que você vai utilizar estão localizados em: \n",
    "\n",
    "https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv \n",
    "\n",
    " \n",
    "\n",
    "Na coluna 'review_title' você vai encontrar feedbacks passados dos nossos clientes em nossos produtos e, na coluna 'overall_rating', a nota que foi dada. Esse é o único dado que temos para auxiliar na criação desse modelo de previsões. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTAR AS BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# tokeniza\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type:ignore\n",
    "# Ajusta o tamanho do vetor\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  #type:ignore\n",
    "# Define o modelo de rede neural utilizada\n",
    "from tensorflow.keras.models import Sequential #type:ignore\n",
    "# Camadas da rede neural\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout #type:ignore\n",
    "# Otimizador de taxa de aprendizado\n",
    "from tensorflow.keras.optimizers import Adam #type:ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OBTER DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     ENDERECO_DADOS \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(ENDERECO_DADOS, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)[[\u001b[39m'\u001b[39m\u001b[39mreview_title\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moverall_rating\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      6\u001b[0m     \u001b[39m#print(df.columns)                \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:389\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m             \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[0;32m    388\u001b[0m             compression \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 389\u001b[0m         reader \u001b[39m=\u001b[39m BytesIO(req\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m    390\u001b[0m     \u001b[39mreturn\u001b[39;00m IOArgs(\n\u001b[0;32m    391\u001b[0m         filepath_or_buffer\u001b[39m=\u001b[39mreader,\n\u001b[0;32m    392\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m         mode\u001b[39m=\u001b[39mfsspec_mode,\n\u001b[0;32m    396\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[39mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\http\\client.py:482\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 482\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[0;32m    483\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m    484\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\http\\client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[0;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[0;32m    633\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'\n",
    "\n",
    "    df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_title', 'overall_rating']]\n",
    "\n",
    "    \n",
    "    #print(df.columns)                \n",
    "except Exception as e:\n",
    "    print('ERRO AO OBTER DADOS', e)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRATAR DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 1 2 3]\n",
      "overall_rating\n",
      "5    47856\n",
      "4    32285\n",
      "1    27280\n",
      "3    16279\n",
      "2     8371\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Excluindo valores nao existente (nans)\n",
    "    df = df.dropna(subset=['review_title', 'overall_rating'])\n",
    "\n",
    "    #Transformando colunas em arrays\n",
    "    texts = np.array(df['review_title'])\n",
    "    rating = np.array(df['overall_rating'])\n",
    "\n",
    "    print(df['overall_rating'].unique())\n",
    "    \n",
    "    # Verifica se os dados estão balanceados entres as CLASSES/CATEGORIAS\n",
    "    # Nesse exmplo a classe 2 tem bem menos dados. \n",
    "    print(df['overall_rating'].value_counts())\n",
    "\n",
    "                     \n",
    "except Exception as e:\n",
    "    print('ERRO AO OBTER DADOS', e)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VETORIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0    3]\n",
      " [   0    0    0 ... 2620   30   16]\n",
      " [   0    0    0 ...  349   45  155]\n",
      " ...\n",
      " [   0    0    0 ...    0    9    1]\n",
      " [   0    0    0 ...    4   19    3]\n",
      " [   0    0    0 ...    1    4   51]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Há bibliotecas de sinonimo para aprimorar o modelo\n",
    "    # Hugging face dicionário de contextos amplos - gratuito\n",
    "    \n",
    "    # Passo 1: tokenizar\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Passo 2: Criar o dicionário\n",
    "    # fit_on_texts: Cria o vacabulário, através do dicionário\n",
    "    # associando cada token a um indice numérico\n",
    "    # lembrando que se a palavra aparecer mais de uma vez, ela vai receber o mesmo indice numerico\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    #print(tokenizer)\n",
    "    # Passo3: Vetorizar, ou seja, transformar os tokens em números a partir do dicionario criado no passo 2\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    "    #print(vetores)\n",
    "    # Passo 4: Padronização do tamanho do vetor - pad\n",
    "    padded_vetores = pad_sequences(vetores) \n",
    "\n",
    "    print(padded_vetores)\n",
    "    \n",
    "                     \n",
    "except Exception as e:\n",
    "    print('ERRO AO VETORIZAR TEXTOS', e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONSTRUIR A REDE NEURAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,936,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">129,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m124\u001b[0m)        │     \u001b[38;5;34m1,936,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m129,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m645\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,067,061</span> (7.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,067,061\u001b[0m (7.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,067,061</span> (7.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,067,061\u001b[0m (7.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo configurado e criado\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Constantes do modelo\n",
    "\n",
    "    # 1ª Constante: Tamanho do vacabulário (tamanho do dicionario do modelo de contexto)\n",
    "    VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # 2ª Constante: Tamanho máximo da sequencia\n",
    "    # É o comprimento máximo de um texto\n",
    "    MAX_SEQUENCE_LENGTH = padded_vetores.shape[1] # linha 0 coluna 1 a quantidade de colunas vai mostrar o comprimento maximo do vetor (a qtde de palavras)\n",
    "\n",
    "    # 3ª Constante: Tamanho do vetor de entrada\n",
    "    # A literatura recomenda que seja iniciado pela quantidade igual a raiz quadrada do tamanho do vocabulário\n",
    "    # Se o volume de dados for de larga escala, pode-se testar iniciando com um tamanho maior\n",
    "    # Se o volume de dados for muito pequeno, pode-se testar iniciando com um tamanho menor\n",
    "    # Cuidado com o overfitting, que é quando o modelo aprende demais e começa a perder a capacidade de generalizar com novos dados\n",
    "    # Não consegue observar todas as diferenças textuais\n",
    "    # Overfitting pode ser observado no treino da rede neural\n",
    "    VETOR_LENGTH = int(np.sqrt(VOCAB_SIZE))\n",
    "\n",
    "    # Inicia-se a construção da rede neural\n",
    "    # Sequential é fluxo linear de camadas (conforme visto na Aula02_RNA.pptx)\n",
    "    # São processadas em ordem\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    # Embedding, na qual os vetores de texto são inseridos\n",
    "    model.add(Embedding(input_dim=VOCAB_SIZE,\n",
    "                        output_dim =VETOR_LENGTH, # Output da camada de entrada. input pra camada oculta\n",
    "                        input_length=MAX_SEQUENCE_LENGTH))\n",
    "    \n",
    "    # Camada oculta ou intermediária\n",
    "    # LSTM - long short term memory, em portugues \" memoria de curto e longo prazo\"\n",
    "    # É onde a magia acontece. É onde o modelo treina baseado nos seus vetores\n",
    "    # Números de unidades de memória, que é a quantidade de neuronios. Quanto mais neuronios, maior a acurácia\n",
    "    # No primeiro TESTE experimente somente com uma camada! Cuidado com o overfitting!\n",
    "    # Se for necessário adicionar mais camadas, basta repetir o comando abaixo\n",
    "    \n",
    "    # Primeira camada oculta\n",
    "    model.add(LSTM(128)) # Uma camada com 128 neuronios\n",
    "\n",
    "    # Se necessário adicionar outra camada oculta, repita model.add(LSTM(qtde de neuronios))\n",
    "\n",
    "    # Camada de saída - Camada densa\n",
    "    # Na classificação precisa ajustar para a quantidade de CLASSES/categorias de saída\n",
    "    # Em RNA DE CLASSIFICAO (CLASSES/CATEGORIA)\n",
    "    # a função de ativação mais utilizda é a softmax\n",
    "    # fç de ativação é um calculo matematico\n",
    "    #que vai determinar a saida de cada neuronio\n",
    "    # softmax é aum fç que transforma os valores de saida em PROBABILIDADES que vao de 0 a 1\n",
    "    # a soma das classes, no nosso caso 5 classes, será igual 1\n",
    "    # Aplicar função de ativação para que o resultado das classses de saída somados deem 1 e fique mais facil de ver a saida\n",
    "    # A outra fç mais utilizada é a RELU QUE RETORNA 0 para saidas negativa e o valor original para as saidas maiores do que 0\n",
    "    # o que faz ela ser, mais indicada para modelos de regressao\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Dropout é uma tecnicade regularização do resultado, pra minimizar o overfitting\n",
    "    #ele desabilita neuronios aleatoriamente, justamente para tentar minimizar o overfitting\n",
    "    # cuidado com o underfitting, pq reduzir demais o aprendizado para reduzir o verfitting pode levar a aprender menos d que deveria\n",
    "    model.add(Dropout(0.05)) # desativar 5% dos neuronios aleatoriamente é uma função degenerativa como uma doenca degenerativa, perde neuronios\n",
    "\n",
    "    # Construir o modelo\n",
    "    # É literalmente pegar as definições anteriores e construir o modelo\n",
    "    # input_shape: é o formato dos dados de entrada e ainda o tamanho máximo do texto (MAX_SEQUENCE_LENGHT)\n",
    "\n",
    "    model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    # Otimizador de taxa de aprendizado. \n",
    "    # Importante para ajustar em casos de overfitting\n",
    "    # Adam é o otimizador que ajusta essa taxa de aprendizado\n",
    "    # parametro learn_rating: Quanto menor, melhor o aprendizado. Menos risco de overfitting \n",
    "    otimizador = Adam(learning_rate=0.0001)\n",
    "\n",
    "    # Compilar o modelo\n",
    "    # Verificar se há ou não algum erro\n",
    "    # É informado o otimizador e a métrica de perda (LOSS)\n",
    "    # loss - erro quadro médio (mean_squared_error)\n",
    "    model.compile(optimizer=otimizador, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    model.summary()\n",
    "    print('Modelo configurado e criado')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print('ERRO AO CONSTRUIR A REDE NEURAL', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TREINAR O MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 15ms/step - loss: 11.3316 - val_loss: 1.2235\n",
      "Epoch 2/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 15ms/step - loss: 9.9711 - val_loss: 1.2000\n",
      "Epoch 3/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - loss: 9.9085 - val_loss: 1.0703\n",
      "Epoch 4/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 16ms/step - loss: 9.5566 - val_loss: 1.1210\n",
      "Epoch 5/5\n",
      "\u001b[1m3302/3302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - loss: 9.4984 - val_loss: 1.1004\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_vetores,\n",
    "        rating,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Qdo a gente constroi um RNA de classificação\n",
    "    # Se o meu dado tiver 0 na classe, eu não preciso ajustar, mas se começar em 1, precisa desse ajuste\n",
    "    # Os rótulos que a RNA atribui [0,1,2,3,4] e os meus dados [1,2,3,4,5] \n",
    "    y_train_adjusted = y_train - 1\n",
    "    y_test_adjusted = y_test - 1\n",
    "\n",
    "    # Aplicar pesos às categorias\n",
    "    # Verifica o tamanho do dataframe e divide pelo tamanho das categorias.\n",
    "    # quanto menor a quantidade de dados, maiores os pesos, justamente para eles conseguirem ser mais expressivos\n",
    "    pesos = len(df['overall_rating'])/df['overall_rating'].value_counts()\n",
    "    #print(pesos)\n",
    "\n",
    "    # O treino da rede neural\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_adjusted,\n",
    "        epochs=5,\n",
    "        batch_size=32, # Quanto maior o tamanho do batch, menor o aprendizado, porém evita overfitting\n",
    "        # Quanto menor o tamnho da batch o modelo tende a um maior aprendizado (logica: ler um livro mais lento, vc compreende melhor)\n",
    "        class_weight = pesos.to_dict(),\n",
    "        validation_data=(X_test, y_test_adjusted))\n",
    "    \n",
    "    # a loss precisa ficar sempre reduzindo, se em algumm momento alumentar, tem um sinal de overfitting\n",
    "    # Na regressão precisa comparar a loss com a val_loss\n",
    "    \n",
    "except Exception as e:\n",
    "    print('ERRO AO TREINAR O MODELO', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESTAR O MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Previsões: [[0.0001 0.0004 0.0085 0.2633 0.7277]\n",
      " [0.3491 0.2292 0.1714 0.1456 0.1048]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    novos_textos = [\n",
    "        'Muito bom, gostei bastante. Top demais! Compensa muito!',\n",
    "         \"Muito bom, Americanas. Só faz besteira. Não recomento\",\n",
    "    ]\n",
    "\n",
    "    novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "    novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "\n",
    "    predicoes = model.predict(novas_sequencias_padded)\n",
    "\n",
    "    # Formatar valores de saida\n",
    "    np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "    print(\"Previsões:\", predicoes) # precisa somar um para voltar as classes normais predicoes + 1 se nao usar a função de ativação\n",
    "    \n",
    "except Exception as e:\n",
    "    print('ERRO AO TESTAR O MODELO', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
