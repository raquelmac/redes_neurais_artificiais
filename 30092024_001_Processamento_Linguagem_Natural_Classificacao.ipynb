{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXEMPLO 02** <br>\n",
    "Você recebeu um convie para uma consultoria, na qual deve desenvolver um modelo de previsões de feedbacks de clientes em produtos comprados na loja, que serão serão coletados do instagram\n",
    "\n",
    "Os dados que você vai utilizar estão localizados em:\n",
    "https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv\n",
    "\n",
    "Na coluna 'review_title' você vai encontrar feedbacks passados dos nossos clientes em nossos produtos e, na coluna 'overall_rating', a nota que foi dada. Esse é o único dado que temos para auxilair na criação desse modelo de previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas\n",
    "# tensorflow é uma biblioteca de código aberto para aprendizado de máquina \n",
    "# e aprendizado profundopara treinamento e inferência de redes neurais artificiais.\n",
    "# https://www.tensorflow.org/\n",
    "# %pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# constante do endereco do arquivo\n",
    "ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtendo dados...\n",
      "reviewer_state\n",
      "SP    49207\n",
      "RJ    17580\n",
      "MG    16322\n",
      "PR     6745\n",
      "RS     6622\n",
      "BA     4806\n",
      "SC     4478\n",
      "ES     3052\n",
      "PE     2955\n",
      "GO     2532\n",
      "DF     2395\n",
      "CE     2171\n",
      "PA     1497\n",
      "MA     1218\n",
      "MT      990\n",
      "RN      926\n",
      "PB      885\n",
      "MS      834\n",
      "AL      729\n",
      "PI      699\n",
      "SE      537\n",
      "TO      397\n",
      "RO      327\n",
      "AM      208\n",
      "AP      113\n",
      "AC       87\n",
      "RR       70\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Local\\Temp\\ipykernel_19880\\2188012562.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ENDERECO_DADOS,sep=',',encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "# obter dados\n",
    "try:\n",
    "    print('Obtendo dados...')\n",
    "    df = pd.read_csv(ENDERECO_DADOS,sep=',',encoding='utf-8')\n",
    "    print(df['reviewer_state'].value_counts())\n",
    "    #print(df.shape)\n",
    "    df = df[df['reviewer_state'] == \"PR\"][['review_text','overall_rating']] \n",
    "    \n",
    "    \n",
    "    #print(df.head()) \n",
    "    #print(df.shape)\n",
    "    \n",
    "    #print(df.columns)\n",
    "\n",
    "    \n",
    "    # delimitando colunas para criação do modelo\n",
    "    df = df.dropna(subset=['review_text','overall_rating'])\n",
    "\n",
    "    # transformando colunas em arrays\n",
    "    texts = np.array(df['review_text'])\n",
    "    ratings = np.array(df['overall_rating'])  \n",
    "    \n",
    "    #print(df.head())\n",
    "    #print(len(df))\n",
    "    #print(df['overall_rating'].unique())\n",
    "    #print(df['overall_rating'].value_counts())\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao obter dados: ',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetorizando texto...\n",
      "[[    0     0     0 ...   274     4   142]\n",
      " [    0     0     0 ...    31   152    15]\n",
      " [    0     0     0 ...    27    27    27]\n",
      " ...\n",
      " [    0     0     0 ...    53     3    53]\n",
      " [    0     0     0 ...     9    23 10568]\n",
      " [    0     0     0 ...   676    66   408]]\n",
      "Texto vetorizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vetorização de texto\n",
    "try:\n",
    "    print('Vetorizando texto...')\n",
    "\n",
    "    # Passo 1: Tokenizar os textos\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit_on_texts: cria o vocabulário\n",
    "    # vocabulário: dicionário que mapeia palavras para números\n",
    "    # cada palavra é mapeada para um número inteiro e \n",
    "    # esse número inteiro é o índice da palavra no vocabulário\n",
    "    # toda vez que uma palavra aparece em um texto,\n",
    "    # ela é substituída pelo seu índice no vocabulário\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Converter os textos em sequências de números\n",
    "    # texts_to_sequences: converte os textos em sequências de números\n",
    "    # É a vetorização dos textos. Cada texto é convertido em um\n",
    "    # vetor de números inteiros, onde cada número inteiro representa\n",
    "    # uma palavra do texto, baseada no índice da palavra no vocabulário\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Passo 2: Padronizar o comprimento das sequências (deixar todas com o mesmo tamanho)\n",
    "    padded_vetores = pad_sequences(vetores)  # Deixar todas as sequências com tamanho 10\n",
    "\n",
    "    # Verificar o resultado da tokenização e padding\n",
    "    print(padded_vetores)\n",
    "\n",
    "    print('Texto vetorizado com sucesso!')\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao vetorizar texto: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construindo a rede neural...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "# Definir as constantes para o modelo\n",
    "try:\n",
    "                print('Construindo a rede neural...')\n",
    "                # tamanho do vocabulário\n",
    "                VOCAB_SIZE = len(tokenizer.word_index) + 1  \n",
    "\n",
    "                # Comprimento máximo da sequência\n",
    "                MAX_SEQUENCE_LENGTH = padded_vetores.shape[1]\n",
    "\n",
    "                # Tamanho do vetor\n",
    "                VETOR_LENGHT = int(np.sqrt(VOCAB_SIZE))\n",
    "\n",
    "                # Construir o modelo sequencial\n",
    "                model = Sequential()\n",
    "\n",
    "                # Camada de entrada\n",
    "                model.add(\n",
    "                        Embedding(\n",
    "                                input_dim=VOCAB_SIZE, \n",
    "                                output_dim=VETOR_LENGHT, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH\n",
    "                                )\n",
    "                )\n",
    "\n",
    "                # Camada LSTM para capturar a sequência do texto\n",
    "                # rodar primeiro somente com 128\n",
    "                model.add(LSTM(128))\n",
    "\n",
    "                # rodar com mais camadas\n",
    "                # return_sequences: parâmetro que indica se a camada deve retornar\n",
    "                # a sequência inteira para a próxima camada\n",
    "                # sequencia inteira é a sequência de saídas de cada unidade de tempo\n",
    "                #model.add(LSTM(32)) # SÓ UTILIZA O RETURN SEQUENCE SE ACRESCENTAR CAMADAS\n",
    "\n",
    "                #model.add(LSTM(64, return_sequences=True))\n",
    "\n",
    "                # última camada LSTM não precisa retornar a sequência inteira\n",
    "                #model.add(LSTM(32))\n",
    "\n",
    "\n",
    "                # Camada final para prever a nota (classificação)\n",
    "                # Definir a quantidade de neurônios de acordo \n",
    "                # com a quantidade de classes/categorias\n",
    "                model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "                # construir o modelo\n",
    "                model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "                # Usar um otimizador Adam com uma taxa de aprendizado menor\n",
    "                # Adam: é um otimizador que ajusta a taxa de aprendizado\n",
    "                otimizador = Adam(learning_rate=0.0001) \n",
    "\n",
    "                # Compilar o modelo\n",
    "                # loss precisa ser ajustado de acordo com o tipo de problema\n",
    "                # nesse caso, é um problema de classificação\n",
    "                # sparse_categorical_crossentropy: função de perda para classificação\n",
    "                # de múltiplas classes/categorias\n",
    "                # sparse: indica que os rótulos são inteiros\n",
    "                # categorical: indica que os rótulos são one-hot encoded, ou seja,\n",
    "                # cada rótulo é uma categoria\n",
    "                # crossentropy: é a função de perda que mede a diferença entre a distribuição\n",
    "                # de probabilidade prevista pelo modelo e a distribuição de probabilidade real\n",
    "                # nesse caso observa-se q a perda durante as épocas\n",
    "                # Se a perda estiver diminuindo, o modelo está aprendendo\n",
    "                # Se a perda estiver aumentando, o modelo não está aprendendo e pode estar\n",
    "                # ocorrendo overfitting\n",
    "                model.compile(optimizer=otimizador, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "                # Verificar o resumo do modelo\n",
    "                #model.summary()\n",
    "except Exception as e:\n",
    "                print('Erro ao construir a rede neural: ',e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando a rede neural...\n",
      "Epoch 1/6\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 112ms/step - loss: 0.4127 - val_loss: 1.5822\n",
      "Epoch 2/6\n",
      "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 112ms/step - loss: 0.3719 - val_loss: 1.6675\n",
      "Epoch 3/6\n",
      "\u001b[1m282/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 0.3590"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 27\u001b[0m\n\u001b[0;32m     20\u001b[0m                 pesos \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39moverall_rating\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m/\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39moverall_rating\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n\u001b[0;32m     22\u001b[0m                 \u001b[39m# Aumentar o número de épocas para dar mais tempo ao modelo para aprender\u001b[39;00m\n\u001b[0;32m     23\u001b[0m                 \u001b[39m# Aumentar o tamanho do batch para acelerar o treinamento, porém,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m                 \u001b[39m# pode ser que o modelo não aprenda bem\u001b[39;00m\n\u001b[0;32m     25\u001b[0m                 \u001b[39m# Reduzir o tamanho do batch para dar mais tempo ao modelo para aprender\u001b[39;00m\n\u001b[0;32m     26\u001b[0m                 \u001b[39m# e para evitar overfitting e aprender melhor\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m                 model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     28\u001b[0m                         X_train, \u001b[39m# textos de treino \u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m                         y_train_adjusted, \u001b[39m# notas de treino\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m                         epochs\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, \u001b[39m# número de épocas \u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m                         batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, \u001b[39m# tamanho do batch de treino\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m                         \u001b[39m#class_weight = pesos.to_dict(),\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m                         validation_data\u001b[39m=\u001b[39;49m(X_test, y_test_adjusted)\n\u001b[0;32m     34\u001b[0m                         )\n\u001b[0;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     37\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mErro ao treinar a rede neural: \u001b[39m\u001b[39m'\u001b[39m,e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# treinar a rede neural\n",
    "try:\n",
    "                print('Treinando a rede neural...')\n",
    "\n",
    "                # Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                padded_vetores, # textos vetorizados\n",
    "                                                ratings, # notas\n",
    "                                                test_size=0.2, # 20% dos dados para teste\n",
    "                                                random_state=42) # seed para reprodução dos resultados\n",
    "\n",
    "                # Ajuste temporário dos rótulos para o intervalo [0, 4] para o treinamento\n",
    "                # Visto que a classificação começa em 1 e termina em 5\n",
    "                # e a classificação começa em 0\n",
    "                y_train_adjusted = y_train - 1\n",
    "                y_test_adjusted = y_test - 1\n",
    "\n",
    "                #peso\n",
    "                pesos = len(df['overall_rating'])/df['overall_rating'].value_counts()\n",
    "\n",
    "                # Aumentar o número de épocas para dar mais tempo ao modelo para aprender\n",
    "                # Aumentar o tamanho do batch para acelerar o treinamento, porém,\n",
    "                # pode ser que o modelo não aprenda bem\n",
    "                # Reduzir o tamanho do batch para dar mais tempo ao modelo para aprender\n",
    "                # e para evitar overfitting e aprender melhor\n",
    "                model.fit(\n",
    "                        X_train, # textos de treino \n",
    "                        y_train_adjusted, # notas de treino\n",
    "                        epochs=6, # número de épocas \n",
    "                        batch_size=16, # tamanho do batch de treino\n",
    "                        #class_weight = pesos.to_dict(),\n",
    "                        validation_data=(X_test, y_test_adjusted)\n",
    "                        )\n",
    "                \n",
    "except Exception as e:\n",
    "                print('Erro ao treinar a rede neural: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando previsões...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Previsões: [[0.     0.     0.004  0.1313 0.8646]\n",
      " [0.     0.     0.0024 0.0816 0.916 ]\n",
      " [0.0165 0.0163 0.2403 0.5304 0.1965]]\n"
     ]
    }
   ],
   "source": [
    "# realizar previsões\n",
    "try:\n",
    "                print('Realizando previsões...')\n",
    "                # Exemplos de novos textos para testar o modelo\n",
    "                novos_textos = [\n",
    "                    \"Simplesmente adorei! O produto superou minhas expectativas em todos os aspectos. A qualidade é excelente e os resultados são visíveis em pouco tempo. A embalagem é prática e o atendimento ao cliente foi impecável. Recomendo a todos que buscam um produto eficaz e confiável!\",\n",
    "                    \"Incrível! Estou impressionada com a eficácia deste produto. A textura é suave e o cheiro é maravilhoso. Usei diariamente e já notei grandes melhorias. Além disso, o custo-benefício é excelente. Sem dúvida, voltarei a comprar e indicarei para amigos. Muito satisfeita!\",\n",
    "                    \"Perfeito! Este produto mudou minha rotina. A aplicação é fácil e rápida, e os resultados são surpreendentes. Senti a diferença logo nas primeiras semanas. A marca realmente se preocupa com a qualidade. Não consigo imaginar meu dia a dia sem ele. Altamente recomendado!\"\n",
    "                ]\n",
    "\n",
    "                # Vetorizar e padronizar os novos textos\n",
    "                novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "                novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "\n",
    "                # Fazer previsões\n",
    "                predicoes = model.predict(novas_sequencias_padded)\n",
    "\n",
    "                # Formatar valores de saída\n",
    "                np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "                # Exibir as previsões\n",
    "                print(\"Previsões:\\n\", predicoes)\n",
    "except Exception as e:\n",
    "                print('Erro ao realizar previsões: ',e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
