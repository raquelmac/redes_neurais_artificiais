{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo 02: Previsão de feedbacks de produtos B2W** \n",
    "\n",
    " \n",
    "\n",
    "Você recebeu um convite para uma consultoria, na qual deve desenvolver um modelo de previsões de feedbacks de clientes em produtos comprados na loja, que serão coletados do instagram. \n",
    "\n",
    " \n",
    "\n",
    "Os dados que você vai utilizar estão localizados em: \n",
    "\n",
    "https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv \n",
    "\n",
    " \n",
    "\n",
    "Na coluna 'review_title' você vai encontrar feedbacks passados dos nossos clientes em nossos produtos e, na coluna 'overall_rating', a nota que foi dada. Esse é o único dado que temos para auxiliar na criação desse modelo de previsões. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTAR AS BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# tokeniza\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type:ignore\n",
    "# Ajusta o tamanho do vetor\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  #type:ignore\n",
    "# Define o modelo de rede neural utilizada\n",
    "from tensorflow.keras.models import Sequential #type:ignore\n",
    "# Camadas da rede neural\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense #type:ignore\n",
    "# Otimizador de taxa de aprendizado\n",
    "from tensorflow.keras.optimizers import Adam #type:ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OBTER DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       submission_date                                        reviewer_id  \\\n",
      "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
      "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
      "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
      "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
      "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
      "\n",
      "  product_id                                       product_name  \\\n",
      "0  132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
      "1   22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
      "2  113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
      "3  113851581               Betoneira Columbus - Roma Brinquedos   \n",
      "4  131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
      "\n",
      "    product_brand      site_category_lv1       site_category_lv2  \\\n",
      "0             NaN            Informática                Notebook   \n",
      "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
      "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
      "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
      "4              lg      TV e Home Theater                      TV   \n",
      "\n",
      "                       review_title  overall_rating recommend_to_a_friend  \\\n",
      "0                               Bom               4                   Yes   \n",
      "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
      "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
      "3        presente mais que desejado               4                   Yes   \n",
      "4            Sem duvidas, excelente               5                   Yes   \n",
      "\n",
      "                                         review_text  reviewer_birth_year  \\\n",
      "0  Estou contente com a compra entrega rápida o ú...               1958.0   \n",
      "1  Por apenas R$1994.20,eu consegui comprar esse ...               1996.0   \n",
      "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               1984.0   \n",
      "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               1985.0   \n",
      "4  A entrega foi no prazo, as americanas estão de...               1994.0   \n",
      "\n",
      "  reviewer_gender reviewer_state  \n",
      "0               F             RJ  \n",
      "1               M             SC  \n",
      "2               M             SP  \n",
      "3               F             SP  \n",
      "4               M             MG  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Local\\Temp\\ipykernel_25428\\2253079434.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')#[['review_title', 'overall_rating']]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'\n",
    "\n",
    "    df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_title', 'overall_rating']]\n",
    "    #print(df.head(5))\n",
    "                     \n",
    "except Exception as e:\n",
    "    print('ERRO AO OBTER DADOS', e)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRATAR DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       review_title  overall_rating\n",
      "0                               Bom               4\n",
      "1  Preço imbatível, ótima qualidade               4\n",
      "2      ATENDE TODAS AS EXPECTATIVA.               4\n",
      "3        presente mais que desejado               4\n",
      "4            Sem duvidas, excelente               5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Excluindo valores nao existente (nans)\n",
    "    df = df.dropna(subset=['review_title', 'overall_rating'])\n",
    "\n",
    "    #Transformando colunas em arrays\n",
    "    texts = np.array(df['review_title'])\n",
    "    rating = np.array(df['overall_rating'])\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "                     \n",
    "except Exception as e:\n",
    "    print('ERRO AO OBTER DADOS', e)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VETORIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0    3]\n",
      " [   0    0    0 ... 2620   30   16]\n",
      " [   0    0    0 ...  349   45  155]\n",
      " ...\n",
      " [   0    0    0 ...    0    9    1]\n",
      " [   0    0    0 ...    4   19    3]\n",
      " [   0    0    0 ...    1    4   51]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Passo 1: tokenizar\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Passo 2: Criar o dicionário\n",
    "    # fit_on_texts: Cria o vacabulário, através do dicionário\n",
    "    # associando cada token a um indice numérico\n",
    "    # lembrando que se a palavra aparecer mais de uma vez, ela vai receber o mesmo indice numerico\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Passo3: Vetorizar, ou seja, transformar os tokens em números a partir do dicionario criado no passo 2\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Passo 4: Padronização do tamanho do vetor - pad\n",
    "    padded_vetores = pad_sequences(vetores) \n",
    "\n",
    "    print(padded_vetores)\n",
    "    \n",
    "                     \n",
    "except Exception as e:\n",
    "    print('ERRO AO VETORIZAR TEXTOS', e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONSTRUIR A REDE NEURAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,936,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">129,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m124\u001b[0m)        │     \u001b[38;5;34m1,936,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m129,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,066,545</span> (7.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,066,545\u001b[0m (7.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,066,545</span> (7.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,066,545\u001b[0m (7.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo configurado e criado\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Constantes do modelo\n",
    "\n",
    "    # 1ª Constante: Tamanho do vacabulário (tamanho do dicionario do modelo de contexto)\n",
    "    VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # 2ª Constante: Tamanho máximo da sequencia\n",
    "    # É o comprimento máximo de um texto\n",
    "    MAX_SEQUENCE_LENGTH = padded_vetores.shape[1] # linha 0 coluna 1 a quantidade de colunas vai mostrar o comprimento maximo do vetor (a qtde de palavras)\n",
    "\n",
    "    # 3ª Constante: Tamanho do vetor de entrada\n",
    "    # A literatura recomenda que seja iniciado pela quantidade igual a raiz quadrada do tamanho do vocabulário\n",
    "    # Se o volume de dados for de larga escala, pode-se testar iniciando com um tamanho maior\n",
    "    # Se o volume de dados for muito pequeno, pode-se testar iniciando com um tamanho menor\n",
    "    # Cuidado com o overfitting, que é quando o modelo aprende demais e começa a perder a capacidade de generalizar com novos dados\n",
    "    # Não consegue observar todas as diferenças textuais\n",
    "    # Overfitting pode ser observado no treino da rede neural\n",
    "    VETOR_LENGTH = int(np.sqrt(VOCAB_SIZE))\n",
    "\n",
    "    # Inicia-se a construção da rede neural\n",
    "    # Sequential é fluxo linear de camadas (conforme visto na Aula02_RNA.pptx)\n",
    "    # São processadas em ordem\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    # Embedding, na qual os vetores de texto são inseridos\n",
    "    model.add(Embedding(input_dim=VOCAB_SIZE,\n",
    "                        output_dim =VETOR_LENGTH, # Output da camada de entrada. input pra camada oculta\n",
    "                        input_length=MAX_SEQUENCE_LENGTH))\n",
    "    \n",
    "    # Camada oculta ou intermediária\n",
    "    # LSTM - long short term memory, em portugues \" memoria de curto e longo prazo\"\n",
    "    # É onde a magia acontece. É onde o modelo treina baseado nos seus vetores\n",
    "    # Números de unidades de memória, que é a quantidade de neuronios. Quanto mais neuronios, maior a acurácia\n",
    "    # No primeiro TESTE experimente somente com uma camada! Cuidado com o overfitting!\n",
    "    # Se for necessário adicionar mais camadas, basta repetir o comando abaixo\n",
    "    \n",
    "    # Primeira camada oculta\n",
    "    model.add(LSTM(128)) # Uma camada com 128 neuronios\n",
    "\n",
    "    # Se necessário adicionar outra camada oculta, repita model.add(LSTM(qtde de neuronios))\n",
    "\n",
    "    # Camada de saída - Camada densa\n",
    "    # Regressão que é ocaso desse exemplo. Somente 1 camada\n",
    "    model.add(Dense(1))\n",
    "    # Construir o modelo\n",
    "    # É literalmente pegar as definições anteriores e construir o modelo\n",
    "    # input_shape: é o formato dos dados de entrada e ainda o tamanho máximo do texto (MAX_SEQUENCE_LENGHT)\n",
    "\n",
    "    model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    # Otimizador de taxa de aprendizado. \n",
    "    # Importante para ajustar em casos de overfitting\n",
    "    # Adam é o otimizador que ajusta essa taxa de aprendizado\n",
    "    # parametro learn_rating: Quanto menor, melhor o aprendizado. Menos risco de overfitting\n",
    "    otimizador = Adam(learning_rate=0.0005)\n",
    "\n",
    "    # Compilar o modelo\n",
    "    # Verificar se há ou não algum erro\n",
    "    # É informado o otimizador e a métrica de perda (LOSS)\n",
    "    # loss - erro quadro médio (mean_squared_error)\n",
    "    model.compile(optimizer=otimizador, loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    print('Modelo configurado e criado')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print('ERRO AO CONSTRUIR A REDE NEURAL', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TREINAR O MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 35ms/step - loss: 1.9668 - val_loss: 0.7093\n",
      "Epoch 2/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - loss: 0.6182 - val_loss: 0.6730\n",
      "Epoch 3/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - loss: 0.5504 - val_loss: 0.6590\n",
      "Epoch 4/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - loss: 0.5019 - val_loss: 0.6503\n",
      "Epoch 5/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - loss: 0.4689 - val_loss: 0.6498\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_vetores,\n",
    "        rating,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # O treino da rede neural\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_test, y_test))\n",
    "    \n",
    "except Exception as e:\n",
    "    print('ERRO AO TREINAR O MODELO', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESTAR O MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Previsões: [[3.0520608 ]\n",
      " [0.13965307]\n",
      " [3.8089793 ]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    novos_textos = [\n",
    "        'Muito bom, gostei bastante. Top demais! Compensa muito!',\n",
    "        \"Não recomendo, péssimo produto. Não funciona\",\n",
    "        \"Muito bom, Americanas. Só faz merda\"\n",
    "    ]\n",
    "\n",
    "    novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "    novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "\n",
    "    predicoes = model.predict(novas_sequencias_padded)\n",
    "    print(\"Previsões:\", predicoes)\n",
    "    \n",
    "except Exception as e:\n",
    "    print('ERRO AO TESTAR O MODELO', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
